{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4846,"status":"ok","timestamp":1694396340846,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"KeGESTERl5cH","outputId":"a6a477b1-261a-45fd-bce8-f0cd0cfc1128"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["## Connect to gg driver\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694396340847,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"nq60EXb1mDGi","outputId":"7f73647f-99c8-42c7-beeb-58321e8bdf8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1g_hBCGxmI5lTFXyvD-igJcroxCjDzObt/AI Tutor/CV/Topic2: Object Classification/day06\n"]}],"source":["%cd /content/drive/MyDrive/AI Tutor/CV/Topic2: Object Classification/day06"]},{"cell_type":"markdown","metadata":{"id":"DAhekoUDuOrN"},"source":["### Common function usage\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":26405,"status":"ok","timestamp":1694396367247,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"P_6S5htj07-S"},"outputs":[],"source":["%%capture\n","!pip install onnxruntime\n","!pip install onnxruntime-gpu\n","!pip install netron\n","!pip install onnx\n","!pip install pyngrok"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1869,"status":"ok","timestamp":1694396369114,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"O8jRs4pSmV3r"},"outputs":[],"source":["import torch\n","import onnxruntime\n","import numpy as np\n","import time\n","def load_onnx_model(path_onnx, providers=['CUDAExecutionProvider', 'CPUExecutionProvider']):\n","    # Create an ONNX Runtime inference session for the ONNX model\n","    ort_session = onnxruntime.InferenceSession(\n","        path_onnx,\n","        providers=providers\n","        )\n","    return ort_session\n","\n","def onnx_infer(ort_session, input_data):\n","    ort_inputs = {ort_session.get_inputs()[0].name: input_data}\n","    ort_output = ort_session.run(None, ort_inputs)\n","    return ort_output"]},{"cell_type":"markdown","metadata":{"id":"W6o71UCzubDI"},"source":["## Estimate perfomances"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2950,"status":"ok","timestamp":1694396372057,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"zrKStPmVucg6"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\DAT NGUYEN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n","  warnings.warn(\n"]}],"source":["model = torch.load(\"models/torch/resnet18.pth\")\n","onnx_float16 = load_onnx_model('models/onnx/vgg19_float16bit.onnx')\n","onnx_float32 = load_onnx_model('models/onnx/vgg19_float32bit.onnx')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10489,"status":"ok","timestamp":1694396382542,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"ngSSkYRXikFc","outputId":"fc50984f-3feb-43da-d742-0e1e745e07ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 37.5815 ms\n","Batch size 4: 122.9063 ms\n","Batch size 8: 259.3822 ms\n","Batch size 16: 489.7148 ms\n","Batch size 32: 927.4831 ms\n","Batch size 64: 1795.9056 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 100\n","total_time = 0\n","for batch_size in list_batch_size:\n","    input_data = torch.randn(batch_size, 3, 224, 224)\n","    for _ in range(n_times):\n","        start = time.time()\n","        input_16bit = input_data.half()\n","        input_numpy = input_16bit.numpy()\n","        onnx_infer(onnx_float16, input_numpy)\n","        total_time+= time.time() - start\n","    print(f\"Batch size {batch_size}: {total_time/100*1000:.4f} ms\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12823,"status":"ok","timestamp":1694396395353,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"MfZIjhKJik2Q","outputId":"3915f194-69b1-4e78-c704-1ae553db5b7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 15.1088 ms\n","Batch size 4: 43.4099 ms\n","Batch size 8: 103.1823 ms\n","Batch size 16: 204.0304 ms\n","Batch size 32: 413.2149 ms\n","Batch size 64: 861.7949 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 100\n","total_time = 0\n","for batch_size in list_batch_size:\n","    input_data = torch.randn(batch_size, 3, 224, 224)\n","    for _ in range(n_times):\n","        start = time.time()\n","        input_numpy = input_data.numpy()\n","        onnx_infer(onnx_float32, input_numpy)\n","        total_time+= time.time() - start\n","    print(f\"Batch size {batch_size}: {total_time/100*1000:.4f} ms\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12278,"status":"ok","timestamp":1694396407628,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"fj0fYoBdit6E","outputId":"9a01e3a9-2b13-4c84-e6fd-970f63a2a08f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 10.1238 ms\n","Batch size 4: 16.2336 ms\n","Batch size 8: 26.9664 ms\n","Batch size 16: 47.1026 ms\n","Batch size 32: 84.0582 ms\n","Batch size 64: 160.9605 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 100\n","total_time = 0\n","model.to('cuda')\n","with torch.no_grad():\n","    for batch_size in list_batch_size:\n","        input_data = torch.randn(batch_size, 3, 224, 224)\n","        for _ in range(n_times):\n","            start = time.time()\n","            model(input_data.to('cuda'))\n","            total_time+= time.time() - start\n","        print(f\"Batch size {batch_size}: {total_time/100*1000:.4f} ms\")"]},{"cell_type":"markdown","metadata":{"id":"3f_lnZYN1JV_"},"source":["## Estimate time infer in CPU"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1694396407629,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"p3sxk6xV1Ixb"},"outputs":[],"source":["model = torch.load(\"models/torch/resnet18.pth\").cpu()\n","onnx_float16 = load_onnx_model('models/onnx/vgg19_float16bit.onnx', ['CPUExecutionProvider'])\n","onnx_float32 = load_onnx_model('models/onnx/vgg19_float32bit.onnx', ['CPUExecutionProvider'])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69147,"status":"ok","timestamp":1694396476757,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"MCmU-rfc1Wd1","outputId":"a5829766-ca1e-43d5-ad3b-7ceb17dd334c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 114.1074 ms\n","Batch size 4: 314.1515 ms\n","Batch size 8: 733.3618 ms\n","Batch size 16: 1664.7996 ms\n","Batch size 32: 3378.2198 ms\n","Batch size 64: 6863.7208 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 10\n","total_time = 0\n","for batch_size in list_batch_size:\n","    input_data = torch.randn(batch_size, 3, 224, 224)\n","    for _ in range(n_times):\n","        start = time.time()\n","        input_16bit = input_data.half()\n","        input_numpy = input_16bit.numpy()\n","        onnx_infer(onnx_float16, input_numpy)\n","        total_time+= time.time() - start\n","    print(f\"Batch size {batch_size}: {total_time/n_times*1000:.4f} ms\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44780,"status":"ok","timestamp":1694396521518,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"h4CzvmzU1ZJQ","outputId":"59474504-bee5-4d66-a166-0485ee614041"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 113.3912 ms\n","Batch size 4: 321.8349 ms\n","Batch size 8: 627.6034 ms\n","Batch size 16: 1113.3701 ms\n","Batch size 32: 2289.8710 ms\n","Batch size 64: 4464.4668 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 10\n","total_time = 0\n","for batch_size in list_batch_size:\n","    input_data = torch.randn(batch_size, 3, 224, 224)\n","    for _ in range(n_times):\n","        start = time.time()\n","        input_numpy = input_data.numpy()\n","        onnx_infer(onnx_float32, input_numpy)\n","        total_time+= time.time() - start\n","    print(f\"Batch size {batch_size}: {total_time/n_times*1000:.4f} ms\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73529,"status":"ok","timestamp":1694396595045,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"x_DYH1Iz1cU0","outputId":"680f1dd4-eb7e-4313-8d13-4874605bc776"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size 2: 106.6412 ms\n","Batch size 4: 386.0994 ms\n","Batch size 8: 830.8327 ms\n","Batch size 16: 1551.9976 ms\n","Batch size 32: 3491.1518 ms\n","Batch size 64: 7336.3019 ms\n"]}],"source":["list_batch_size = [2,4,8,16,32,64]\n","n_times = 10\n","total_time = 0\n","model.to('cpu')\n","with torch.no_grad():\n","    for batch_size in list_batch_size:\n","        input_data = torch.randn(batch_size, 3, 224, 224)\n","        for _ in range(n_times):\n","            start = time.time()\n","            model(input_data.to('cpu'))\n","            total_time+= time.time() - start\n","        print(f\"Batch size {batch_size}: {total_time/n_times*1000:.4f} ms\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1694396595046,"user":{"displayName":"Tan Nguyen","userId":"05305622082741944152"},"user_tz":-420},"id":"LNXm9wN81gDG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOKXo+5pb6pAfsssQ8Et45L","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
